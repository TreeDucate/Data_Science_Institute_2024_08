{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5aa5bcc7-14f7-4e72-9e88-4e4c9b50afe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. LOADING AND EXPLORING THE DATA\n",
      "========================================\n",
      "\n",
      "First 5 rows of the dataset:\n",
      "   LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  PAY_1  PAY_2  PAY_3  PAY_4  \\\n",
      "0      20000    2          2         1   24      2      2     -1     -1   \n",
      "1     120000    2          2         2   26     -1      2      0      0   \n",
      "2      90000    2          2         2   34      0      0      0      0   \n",
      "3      50000    2          2         1   37      0      0      0      0   \n",
      "4      50000    1          2         1   57     -1      0     -1      0   \n",
      "\n",
      "   PAY_5  ...  BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  PAY_AMT2  PAY_AMT3  \\\n",
      "0     -2  ...          0          0          0         0       689         0   \n",
      "1      0  ...       3272       3455       3261         0      1000      1000   \n",
      "2      0  ...      14331      14948      15549      1518      1500      1000   \n",
      "3      0  ...      28314      28959      29547      2000      2019      1200   \n",
      "4      0  ...      20940      19146      19131      2000     36681     10000   \n",
      "\n",
      "   PAY_AMT4  PAY_AMT5  PAY_AMT6  default  \n",
      "0         0         0         0        1  \n",
      "1      1000         0      2000        1  \n",
      "2      1000      1000      5000        0  \n",
      "3      1100      1069      1000        0  \n",
      "4      9000       689       679        0  \n",
      "\n",
      "[5 rows x 24 columns]\n",
      "\n",
      "Dataset info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30000 entries, 0 to 29999\n",
      "Data columns (total 24 columns):\n",
      " #   Column     Non-Null Count  Dtype\n",
      "---  ------     --------------  -----\n",
      " 0   LIMIT_BAL  30000 non-null  int64\n",
      " 1   SEX        30000 non-null  int64\n",
      " 2   EDUCATION  30000 non-null  int64\n",
      " 3   MARRIAGE   30000 non-null  int64\n",
      " 4   AGE        30000 non-null  int64\n",
      " 5   PAY_1      30000 non-null  int64\n",
      " 6   PAY_2      30000 non-null  int64\n",
      " 7   PAY_3      30000 non-null  int64\n",
      " 8   PAY_4      30000 non-null  int64\n",
      " 9   PAY_5      30000 non-null  int64\n",
      " 10  PAY_6      30000 non-null  int64\n",
      " 11  BILL_AMT1  30000 non-null  int64\n",
      " 12  BILL_AMT2  30000 non-null  int64\n",
      " 13  BILL_AMT3  30000 non-null  int64\n",
      " 14  BILL_AMT4  30000 non-null  int64\n",
      " 15  BILL_AMT5  30000 non-null  int64\n",
      " 16  BILL_AMT6  30000 non-null  int64\n",
      " 17  PAY_AMT1   30000 non-null  int64\n",
      " 18  PAY_AMT2   30000 non-null  int64\n",
      " 19  PAY_AMT3   30000 non-null  int64\n",
      " 20  PAY_AMT4   30000 non-null  int64\n",
      " 21  PAY_AMT5   30000 non-null  int64\n",
      " 22  PAY_AMT6   30000 non-null  int64\n",
      " 23  default    30000 non-null  int64\n",
      "dtypes: int64(24)\n",
      "memory usage: 5.5 MB\n",
      "None\n",
      "\n",
      "Missing values in the dataset:\n",
      "LIMIT_BAL    0\n",
      "SEX          0\n",
      "EDUCATION    0\n",
      "MARRIAGE     0\n",
      "AGE          0\n",
      "PAY_1        0\n",
      "PAY_2        0\n",
      "PAY_3        0\n",
      "PAY_4        0\n",
      "PAY_5        0\n",
      "PAY_6        0\n",
      "BILL_AMT1    0\n",
      "BILL_AMT2    0\n",
      "BILL_AMT3    0\n",
      "BILL_AMT4    0\n",
      "BILL_AMT5    0\n",
      "BILL_AMT6    0\n",
      "PAY_AMT1     0\n",
      "PAY_AMT2     0\n",
      "PAY_AMT3     0\n",
      "PAY_AMT4     0\n",
      "PAY_AMT5     0\n",
      "PAY_AMT6     0\n",
      "default      0\n",
      "dtype: int64\n",
      "\n",
      "Statistical summary:\n",
      "            LIMIT_BAL           SEX     EDUCATION      MARRIAGE           AGE  \\\n",
      "count    30000.000000  30000.000000  30000.000000  30000.000000  30000.000000   \n",
      "mean    167484.322667      1.603733      1.853133      1.551867     35.485500   \n",
      "std     129747.661567      0.489129      0.790349      0.521970      9.217904   \n",
      "min      10000.000000      1.000000      0.000000      0.000000     21.000000   \n",
      "25%      50000.000000      1.000000      1.000000      1.000000     28.000000   \n",
      "50%     140000.000000      2.000000      2.000000      2.000000     34.000000   \n",
      "75%     240000.000000      2.000000      2.000000      2.000000     41.000000   \n",
      "max    1000000.000000      2.000000      6.000000      3.000000     79.000000   \n",
      "\n",
      "              PAY_1         PAY_2         PAY_3         PAY_4         PAY_5  \\\n",
      "count  30000.000000  30000.000000  30000.000000  30000.000000  30000.000000   \n",
      "mean      -0.016700     -0.133767     -0.166200     -0.220667     -0.266200   \n",
      "std        1.123802      1.197186      1.196868      1.169139      1.133187   \n",
      "min       -2.000000     -2.000000     -2.000000     -2.000000     -2.000000   \n",
      "25%       -1.000000     -1.000000     -1.000000     -1.000000     -1.000000   \n",
      "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "max        8.000000      8.000000      8.000000      8.000000      8.000000   \n",
      "\n",
      "       ...      BILL_AMT4      BILL_AMT5      BILL_AMT6       PAY_AMT1  \\\n",
      "count  ...   30000.000000   30000.000000   30000.000000   30000.000000   \n",
      "mean   ...   43262.948967   40311.400967   38871.760400    5663.580500   \n",
      "std    ...   64332.856134   60797.155770   59554.107537   16563.280354   \n",
      "min    ... -170000.000000  -81334.000000 -339603.000000       0.000000   \n",
      "25%    ...    2326.750000    1763.000000    1256.000000    1000.000000   \n",
      "50%    ...   19052.000000   18104.500000   17071.000000    2100.000000   \n",
      "75%    ...   54506.000000   50190.500000   49198.250000    5006.000000   \n",
      "max    ...  891586.000000  927171.000000  961664.000000  873552.000000   \n",
      "\n",
      "           PAY_AMT2      PAY_AMT3       PAY_AMT4       PAY_AMT5  \\\n",
      "count  3.000000e+04   30000.00000   30000.000000   30000.000000   \n",
      "mean   5.921163e+03    5225.68150    4826.076867    4799.387633   \n",
      "std    2.304087e+04   17606.96147   15666.159744   15278.305679   \n",
      "min    0.000000e+00       0.00000       0.000000       0.000000   \n",
      "25%    8.330000e+02     390.00000     296.000000     252.500000   \n",
      "50%    2.009000e+03    1800.00000    1500.000000    1500.000000   \n",
      "75%    5.000000e+03    4505.00000    4013.250000    4031.500000   \n",
      "max    1.684259e+06  896040.00000  621000.000000  426529.000000   \n",
      "\n",
      "            PAY_AMT6       default  \n",
      "count   30000.000000  30000.000000  \n",
      "mean     5215.502567      0.221200  \n",
      "std     17777.465775      0.415062  \n",
      "min         0.000000      0.000000  \n",
      "25%       117.750000      0.000000  \n",
      "50%      1500.000000      0.000000  \n",
      "75%      4000.000000      0.000000  \n",
      "max    528666.000000      1.000000  \n",
      "\n",
      "[8 rows x 24 columns]\n",
      "\n",
      "Distribution of default payments:\n",
      "default\n",
      "0    23364\n",
      "1     6636\n",
      "Name: count, dtype: int64\n",
      "Default rate: 22.12%\n"
     ]
    }
   ],
   "source": [
    "# Credit Card Default Prediction with Logistic Regression\n",
    "# ======================================================\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('ggplot')\n",
    "sns.set(font_scale=1.2)\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# 1. Load and explore the data\n",
    "# ===========================\n",
    "print(\"1. LOADING AND EXPLORING THE DATA\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('default of credit card clients.csv', sep=';', skiprows=1)\n",
    "\n",
    "# Remove the ID column\n",
    "df = df.iloc[:, 1:]\n",
    "\n",
    "# Rename columns for better readability\n",
    "column_names = [\n",
    "    'LIMIT_BAL', 'SEX', 'EDUCATION', 'MARRIAGE', 'AGE',\n",
    "    'PAY_1', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6',\n",
    "    'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6',\n",
    "    'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6',\n",
    "    'default'\n",
    "]\n",
    "df.columns = column_names\n",
    "\n",
    "# Display the first few rows\n",
    "print(\"\\nFirst 5 rows of the dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# Check data info\n",
    "print(\"\\nDataset info:\")\n",
    "print(df.info())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values in the dataset:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Data statistics\n",
    "print(\"\\nStatistical summary:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Check the distribution of the target variable\n",
    "print(\"\\nDistribution of default payments:\")\n",
    "print(df['default'].value_counts())\n",
    "print(f\"Default rate: {df['default'].mean() * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0251bd0d-f49c-4fe9-978a-c7d89e0c4350",
   "metadata": {},
   "source": [
    "### 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96edf715-e59d-4996-be30-2874b658701f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "2. DATA PREPROCESSING\n",
      "========================================\n",
      "\n",
      "Encoding categorical features...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n2. DATA PREPROCESSING\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Encode categorical features\n",
    "print(\"\\nEncoding categorical features...\")\n",
    "\n",
    "# Map sex: 1=male, 2=female -> 0=male, 1=female\n",
    "df['SEX'] = df['SEX'].map({1: 0, 2: 1})\n",
    "\n",
    "# Map education: 1=graduate, 2=university, 3=high school, 4=others\n",
    "# We'll keep as is but clean the data (some values are 0, 5, 6 which are not in the data description)\n",
    "df['EDUCATION'] = df['EDUCATION'].map(lambda x: 4 if x in [0, 5, 6] else x)\n",
    "\n",
    "# Map marriage: 1=married, 2=single, 3=others\n",
    "# Clean the data (some values are 0 which is not in the data description)\n",
    "df['MARRIAGE'] = df['MARRIAGE'].map(lambda x: 3 if x == 0 else x)\n",
    "\n",
    "# Create dummy variables for categorical features (if needed)\n",
    "# For this tutorial, we'll keep it simple and use the numeric encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cfa770-f6f7-4ced-9ecf-a900d12bf911",
   "metadata": {},
   "source": [
    "### 3. Feature Engineering and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "475bf529-b053-4cb5-a64b-079987465d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "3. FEATURE ENGINEERING AND SELECTION\n",
      "========================================\n",
      "\n",
      "Training set shape: (22500, 27)\n",
      "Testing set shape: (7500, 27)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n3. FEATURE ENGINEERING AND SELECTION\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Create a feature for total bill amount\n",
    "df['TOTAL_BILL'] = df[['BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6']].sum(axis=1)\n",
    "\n",
    "# Create a feature for total payment amount\n",
    "df['TOTAL_PAY'] = df[['PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']].sum(axis=1)\n",
    "\n",
    "# Create a feature for payment ratio (total payment / total bill)\n",
    "df['PAY_RATIO'] = df['TOTAL_PAY'] / df['TOTAL_BILL']\n",
    "df['PAY_RATIO'] = df['PAY_RATIO'].replace([np.inf, -np.inf], 0).fillna(0)\n",
    "\n",
    "# Create a feature for average payment delay\n",
    "df['AVG_PAY_DELAY'] = df[['PAY_1', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']].mean(axis=1)\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop('default', axis=1)\n",
    "y = df['default']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\nTraining set shape: {X_train.shape}\")\n",
    "print(f\"Testing set shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0935dfec-69aa-4be8-84ab-98360441182d",
   "metadata": {},
   "source": [
    "### 4. Model Training: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84889295-056d-4bc8-a80f-68f958918eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "4. MODEL TRAINING: LOGISTIC REGRESSION\n",
      "========================================\n",
      "\n",
      "Training our custom LogisticRegression model...\n",
      "Cost at iteration 500: 0.46402681019559955\n",
      "Cost at iteration 1000: 0.46391625110907125\n",
      "Cost at iteration 1500: 0.463881906608544\n",
      "Cost at iteration 2000: 0.4638625289985032\n",
      "Cost at iteration 2500: 0.46385059653172084\n",
      "Cost at iteration 3000: 0.4638429819015393\n",
      "Cost at iteration 3500: 0.4638379809509058\n",
      "Cost at iteration 4000: 0.4638346091873176\n",
      "Cost at iteration 4500: 0.46383228109929964\n",
      "Cost at iteration 5000: 0.46383063939171343\n",
      "Cost at iteration 5500: 0.463829460346282\n",
      "Cost at iteration 6000: 0.4638286002434648\n",
      "Cost at iteration 6500: 0.4638279644346782\n",
      "Cost at iteration 7000: 0.4638274891188923\n",
      "Cost at iteration 7500: 0.4638271303666043\n",
      "Cost at iteration 8000: 0.4638268573547721\n",
      "Cost at iteration 8500: 0.46382664809774654\n",
      "Cost at iteration 9000: 0.463826486689604\n",
      "Cost at iteration 9500: 0.4638263614823324\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.MyLogisticRegression at 0x298b51cdd30>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\n\\n4. MODEL TRAINING: LOGISTIC REGRESSION\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Define our logistic regression model with explicit implementation\n",
    "class MyLogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, num_iterations=10000, fit_intercept=True):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iterations = num_iterations\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.weights = None\n",
    "        self.intercept = None\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        # Sigmoid function: f(z) = 1 / (1 + e^(-z))\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Add intercept term if needed\n",
    "        if self.fit_intercept:\n",
    "            X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "        \n",
    "        # Initialize parameters\n",
    "        theta = np.zeros(X.shape[1])\n",
    "        m = len(y)\n",
    "        \n",
    "        # Gradient descent\n",
    "        for i in range(self.num_iterations):\n",
    "            # Calculate the hypothesis h(x)\n",
    "            z = np.dot(X, theta)\n",
    "            h = self.sigmoid(z)\n",
    "            \n",
    "            # Calculate the gradient\n",
    "            gradient = np.dot(X.T, (h - y)) / m\n",
    "            \n",
    "            # Update parameters\n",
    "            theta -= self.learning_rate * gradient\n",
    "            \n",
    "            # Optional: Print cost function every 1000 iterations\n",
    "            if (i % 500 == 0) and (i > 0):\n",
    "                cost = self.compute_cost(X, y, theta)\n",
    "                print(f\"Cost at iteration {i}: {cost}\")\n",
    "        \n",
    "        # Save the learned parameters\n",
    "        if self.fit_intercept:\n",
    "            self.intercept = theta[0]\n",
    "            self.weights = theta[1:]\n",
    "        else:\n",
    "            self.weights = theta\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def compute_cost(self, X, y, theta):\n",
    "        m = len(y)\n",
    "        h = self.sigmoid(np.dot(X, theta))\n",
    "        epsilon = 1e-5  # Small value to avoid log(0)\n",
    "        cost = (-1/m) * np.sum(y * np.log(h + epsilon) + (1 - y) * np.log(1 - h + epsilon))\n",
    "        return cost\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        if self.fit_intercept:\n",
    "            X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "        return self.sigmoid(np.dot(X, np.append(self.intercept, self.weights) if self.fit_intercept else self.weights))\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        return (self.predict_proba(X) >= threshold).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "# Train our custom logistic regression model\n",
    "print(\"\\nTraining our custom LogisticRegression model...\")\n",
    "# Using a smaller number of iterations for time constraints\n",
    "my_model = MyLogisticRegression(learning_rate=0.1, num_iterations=10000)\n",
    "my_model.fit(X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7404d779-024d-4b2d-8a95-84f37adff8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For comparison, we'll also use sklearn's LogisticRegression\n",
    "print(\"\\nTraining sklearn's LogisticRegression model...\")\n",
    "sklearn_model = LogisticRegression(max_iter=10000, random_state=42, solver='liblinear')\n",
    "sklearn_model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04338d73-687f-4928-b954-2dd23bbb11ce",
   "metadata": {},
   "source": [
    "### 5. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c1518bd-3feb-4a77-9e6d-a9cb06c4f669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "5. MODEL EVALUATION\n",
      "========================================\n",
      "\n",
      "Classification Report (sklearn model):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.97      0.89      5841\n",
      "           1       0.70      0.24      0.36      1659\n",
      "\n",
      "    accuracy                           0.81      7500\n",
      "   macro avg       0.76      0.61      0.62      7500\n",
      "weighted avg       0.79      0.81      0.77      7500\n",
      "\n",
      "\n",
      "Detailed Metrics:\n",
      "Accuracy: 0.8095\n",
      "Precision: 0.7018\n",
      "Recall: 0.2411\n",
      "F1-Score: 0.3589\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n5. MODEL EVALUATION\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_sklearn = sklearn_model.predict(X_test_scaled)\n",
    "y_prob_sklearn = sklearn_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Slightly different format for our custom model\n",
    "y_pred_custom = my_model.predict(X_test_scaled)\n",
    "y_prob_custom = my_model.predict_proba(X_test_scaled)\n",
    "\n",
    "# We'll use sklearn's results for the metrics\n",
    "print(\"\\nClassification Report (sklearn model):\")\n",
    "print(classification_report(y_test, y_pred_sklearn))\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_sklearn)\n",
    "\n",
    "# Extract values for easier reference\n",
    "tn, fp, fn, tp = conf_matrix.ravel()\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "print(\"\\nDetailed Metrics:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3bb7b9-b61f-4794-8595-995c9ea72cc6",
   "metadata": {},
   "source": [
    "### 6. Feature Importance and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "597fd0b0-75dd-4cfc-9195-8f3d6e15bcec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "6. FEATURE IMPORTANCE AND VISUALIZATION\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n6. FEATURE IMPORTANCE AND VISUALIZATION\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Define a function to visualize the confusion matrix\n",
    "def plot_confusion_matrix(cm, title='Confusion Matrix'):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title(title)\n",
    "    plt.xticks([0.5, 1.5], ['No Default', 'Default'])\n",
    "    plt.yticks([0.5, 1.5], ['No Default', 'Default'])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f8ee240-3c02-484b-b7d7-cc6aed6a175b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Plotting confusion matrix...\n"
     ]
    }
   ],
   "source": [
    "# Plot confusion matrix\n",
    "print(\"\\nPlotting confusion matrix...\")\n",
    "plot_confusion_matrix(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "435fdbc0-523d-4571-a417-2e22563c3eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Plotting feature importance...\n"
     ]
    }
   ],
   "source": [
    "# Get feature importance from the sklearn model\n",
    "feature_importance = np.abs(sklearn_model.coef_[0])\n",
    "feature_names = X.columns\n",
    "\n",
    "# Create a DataFrame for feature importance\n",
    "importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})\n",
    "importance_df = importance_df.sort_values('Importance', ascending=False)\n",
    "\n",
    "# Plot feature importance\n",
    "def plot_feature_importance(importance_df, top_n=10, title='Feature Importance'):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='Importance', y='Feature', data=importance_df.head(top_n))\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('feature_importance.png')\n",
    "    plt.close()\n",
    "\n",
    "print(\"\\nPlotting feature importance...\")\n",
    "plot_feature_importance(importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86e1938c-5e76-401a-967c-22ccff7ddc6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Plotting ROC curve...\n"
     ]
    }
   ],
   "source": [
    "# Plot ROC curve\n",
    "def plot_roc_curve(y_true, y_prob, title='ROC Curve'):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('roc_curve.png')\n",
    "    plt.close()\n",
    "\n",
    "print(\"\\nPlotting ROC curve...\")\n",
    "plot_roc_curve(y_test, y_prob_sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82e4564-e67d-4c87-8bca-595807805bc2",
   "metadata": {},
   "source": [
    "### 7. Visualizing the Impact of Key Factors on Default Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61731e5e-168a-4256-871f-2384a5d417de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "7. VISUALIZING INFLUENCE FACTORS\n",
      "========================================\n",
      "\n",
      "Plotting influence of key factors...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n\\n7. VISUALIZING INFLUENCE FACTORS\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Create a function to plot the impact of key factors\n",
    "def plot_factor_influence(X, y, feature_name, num_bins=10, title=None):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Group the feature into bins and calculate default rate for each bin\n",
    "    bins = pd.cut(X[feature_name], bins=num_bins)\n",
    "    default_rate = y.groupby(bins).mean()\n",
    "    counts = y.groupby(bins).count()\n",
    "    \n",
    "    # Plot default rate by feature bin\n",
    "    ax = default_rate.plot(kind='bar', color='skyblue')\n",
    "    \n",
    "    # Add count labels to each bar\n",
    "    for i, (count, rate) in enumerate(zip(counts, default_rate)):\n",
    "        ax.text(i, rate + 0.02, f'n={count}', ha='center')\n",
    "    \n",
    "    plt.xlabel(feature_name)\n",
    "    plt.ylabel('Default Rate')\n",
    "    plt.title(title or f'Default Rate by {feature_name}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'influence_{feature_name}.png')\n",
    "    plt.close()\n",
    "\n",
    "# Plot the influence of key factors\n",
    "print(\"\\nPlotting influence of key factors...\")\n",
    "key_factors = ['LIMIT_BAL', 'AGE', 'AVG_PAY_DELAY', 'PAY_RATIO']\n",
    "for factor in key_factors:\n",
    "    plot_factor_influence(X, y, factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837d9fcb-9d43-4819-ae72-0af440834efd",
   "metadata": {},
   "source": [
    "### 8. Relationship Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b2a9873-d296-41f4-bde5-e62a58fce8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "8. RELATIONSHIP VISUALIZATION\n",
      "========================================\n",
      "\n",
      "Plotting correlation heatmap...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n8. RELATIONSHIP VISUALIZATION\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Plot correlations between features\n",
    "def plot_correlation_heatmap(df, title='Correlation Matrix'):\n",
    "    plt.figure(figsize=(14, 12))\n",
    "    corr = df.corr()\n",
    "    mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "    sns.heatmap(corr, mask=mask, cmap='coolwarm', annot=False, center=0, square=True)\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('correlation_heatmap.png')\n",
    "    plt.close()\n",
    "\n",
    "print(\"\\nPlotting correlation heatmap...\")\n",
    "plot_correlation_heatmap(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4928fde9-9e25-4e0d-9214-bd303a1e8fb9",
   "metadata": {},
   "source": [
    "### 9. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52561d6a-53de-46a6-8abf-9ecd24b67737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "9. SUMMARY\n",
      "========================================\n",
      "\n",
      "This tutorial demonstrated:\n",
      "1. How to load and preprocess credit card default data\n",
      "2. How to engineer useful features\n",
      "3. How to implement logistic regression from scratch\n",
      "4. How to evaluate model performance with metrics like accuracy, precision, recall, and F1-score\n",
      "5. How to visualize key factors influencing credit card default\n",
      "\n",
      "Key findings:\n",
      "- Payment history (PAY_X columns) are the most important predictors of default\n",
      "- Higher credit limits (LIMIT_BAL) are associated with lower default rates\n",
      "- The model achieved good overall accuracy but struggles with recall for the default class\n",
      "- Feature engineering (like payment ratio and average delay) helped improve model performance\n",
      "\n",
      "\n",
      "Tutorial completed! Check the generated visualizations to better understand the factors affecting credit card default.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n9. SUMMARY\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "print(\"\"\"\n",
    "This tutorial demonstrated:\n",
    "1. How to load and preprocess credit card default data\n",
    "2. How to engineer useful features\n",
    "3. How to implement logistic regression from scratch\n",
    "4. How to evaluate model performance with metrics like accuracy, precision, recall, and F1-score\n",
    "5. How to visualize key factors influencing credit card default\n",
    "\n",
    "Key findings:\n",
    "- Payment history (PAY_X columns) are the most important predictors of default\n",
    "- Higher credit limits (LIMIT_BAL) are associated with lower default rates\n",
    "- The model achieved good overall accuracy but struggles with recall for the default class\n",
    "- Feature engineering (like payment ratio and average delay) helped improve model performance\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nTutorial completed! Check the generated visualizations to better understand the factors affecting credit card default.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b6f1e8-4fda-4290-b7e8-028c6ff61f3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
